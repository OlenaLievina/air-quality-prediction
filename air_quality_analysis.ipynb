{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2290a7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'type' from 'pandas.api' (C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\api\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01murllib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequest\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m pdt\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'type' from 'pandas.api' (C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\pandas\\api\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "from pandas.api import types as pdt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import root_mean_squared_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor, HistGradientBoostingRegressor\n",
    "import xgboost as xgb \n",
    "import joblib\n",
    "import traceback\n",
    "\n",
    "DOWNLOAD_ROOT = 'https://raw.githubusercontent.com/asharvi1/UCI-Air-Quality-Data/master/'\n",
    "AIR_QUALITY_URL = DOWNLOAD_ROOT + 'AirQualityUCI.csv'\n",
    "AIR_QUALITY_DATA_DIR = os.path.join('data', 'air_quality_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1bae3b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with shape (9471, 15) and columns: ['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n"
     ]
    }
   ],
   "source": [
    "def get_air_quality_data(air_quality_url=AIR_QUALITY_URL, aq_data_dir=AIR_QUALITY_DATA_DIR):\n",
    "    \"\"\"Downloads and loads the Air Quality UCI dataset\"\"\"\n",
    "    full_csv_path = os.path.join(aq_data_dir, 'AirQualityUCI.csv')\n",
    "    if not os.path.exists(full_csv_path):\n",
    "        os.makedirs(aq_data_dir, exist_ok=True)\n",
    "        \n",
    "        try:\n",
    "            urllib.request.urlretrieve(air_quality_url, full_csv_path)\n",
    "            print(f\"Data downloaded successfully to {full_csv_path}\")\n",
    "        except Exception as e:\n",
    "            print(f'Failed to download data: {e}')\n",
    "            return pd.DataFrame()  # Return empty DataFrame on failure\n",
    "            \n",
    "    df = pd.read_csv(full_csv_path, sep=';', decimal=',')\n",
    "    df.columns = df.columns.str.strip()\n",
    "    df.dropna(axis=1, how='all', inplace=True)\n",
    "    print(f\"Loaded data with shape {df.shape} and columns: {list(df.columns)}\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b3413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CO(GT)</th>\n",
       "      <th>PT08.S1(CO)</th>\n",
       "      <th>C6H6(GT)</th>\n",
       "      <th>PT08.S2(NMHC)</th>\n",
       "      <th>NOx(GT)</th>\n",
       "      <th>PT08.S3(NOx)</th>\n",
       "      <th>NO2(GT)</th>\n",
       "      <th>PT08.S4(NO2)</th>\n",
       "      <th>PT08.S5(O3)</th>\n",
       "      <th>T</th>\n",
       "      <th>RH</th>\n",
       "      <th>AH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7344.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>7396.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>7393.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "      <td>8991.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.129711</td>\n",
       "      <td>1099.833166</td>\n",
       "      <td>10.083105</td>\n",
       "      <td>939.153376</td>\n",
       "      <td>242.189292</td>\n",
       "      <td>835.493605</td>\n",
       "      <td>112.145137</td>\n",
       "      <td>1456.264598</td>\n",
       "      <td>1022.906128</td>\n",
       "      <td>18.317829</td>\n",
       "      <td>49.234201</td>\n",
       "      <td>1.025530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.436472</td>\n",
       "      <td>217.080037</td>\n",
       "      <td>7.449820</td>\n",
       "      <td>266.831429</td>\n",
       "      <td>206.312007</td>\n",
       "      <td>256.817320</td>\n",
       "      <td>47.629141</td>\n",
       "      <td>346.206794</td>\n",
       "      <td>398.484288</td>\n",
       "      <td>8.832116</td>\n",
       "      <td>17.316892</td>\n",
       "      <td>0.403813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.100000</td>\n",
       "      <td>647.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>383.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>322.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>551.000000</td>\n",
       "      <td>221.000000</td>\n",
       "      <td>-1.900000</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>0.184700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.100000</td>\n",
       "      <td>937.000000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>734.500000</td>\n",
       "      <td>97.000000</td>\n",
       "      <td>658.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>1227.000000</td>\n",
       "      <td>731.500000</td>\n",
       "      <td>11.800000</td>\n",
       "      <td>35.800000</td>\n",
       "      <td>0.736800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.800000</td>\n",
       "      <td>1063.000000</td>\n",
       "      <td>8.200000</td>\n",
       "      <td>909.000000</td>\n",
       "      <td>178.000000</td>\n",
       "      <td>806.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>1463.000000</td>\n",
       "      <td>963.000000</td>\n",
       "      <td>17.800000</td>\n",
       "      <td>49.600000</td>\n",
       "      <td>0.995400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.800000</td>\n",
       "      <td>1231.000000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>1116.000000</td>\n",
       "      <td>321.000000</td>\n",
       "      <td>969.500000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>1674.000000</td>\n",
       "      <td>1273.500000</td>\n",
       "      <td>24.400000</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1.313700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.900000</td>\n",
       "      <td>2040.000000</td>\n",
       "      <td>63.700000</td>\n",
       "      <td>2214.000000</td>\n",
       "      <td>1479.000000</td>\n",
       "      <td>2683.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>2775.000000</td>\n",
       "      <td>2523.000000</td>\n",
       "      <td>44.600000</td>\n",
       "      <td>88.700000</td>\n",
       "      <td>2.231000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            CO(GT)  PT08.S1(CO)     C6H6(GT)  PT08.S2(NMHC)      NOx(GT)  \\\n",
       "count  7344.000000  8991.000000  8991.000000    8991.000000  7396.000000   \n",
       "mean      2.129711  1099.833166    10.083105     939.153376   242.189292   \n",
       "std       1.436472   217.080037     7.449820     266.831429   206.312007   \n",
       "min       0.100000   647.000000     0.100000     383.000000     2.000000   \n",
       "25%       1.100000   937.000000     4.400000     734.500000    97.000000   \n",
       "50%       1.800000  1063.000000     8.200000     909.000000   178.000000   \n",
       "75%       2.800000  1231.000000    14.000000    1116.000000   321.000000   \n",
       "max      11.900000  2040.000000    63.700000    2214.000000  1479.000000   \n",
       "\n",
       "       PT08.S3(NOx)      NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)            T  \\\n",
       "count   8991.000000  7393.000000   8991.000000  8991.000000  8991.000000   \n",
       "mean     835.493605   112.145137   1456.264598  1022.906128    18.317829   \n",
       "std      256.817320    47.629141    346.206794   398.484288     8.832116   \n",
       "min      322.000000     2.000000    551.000000   221.000000    -1.900000   \n",
       "25%      658.000000    77.000000   1227.000000   731.500000    11.800000   \n",
       "50%      806.000000   109.000000   1463.000000   963.000000    17.800000   \n",
       "75%      969.500000   140.000000   1674.000000  1273.500000    24.400000   \n",
       "max     2683.000000   333.000000   2775.000000  2523.000000    44.600000   \n",
       "\n",
       "                RH           AH  \n",
       "count  8991.000000  8991.000000  \n",
       "mean     49.234201     1.025530  \n",
       "std      17.316892     0.403813  \n",
       "min       9.200000     0.184700  \n",
       "25%      35.800000     0.736800  \n",
       "50%      49.600000     0.995400  \n",
       "75%      62.500000     1.313700  \n",
       "max      88.700000     2.231000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_air_quality_clean_data(df=None, target='C6H6(GT)'):\n",
    "    \"\"\"Cleans the air quality dataset by handling missing values and removing unnecessary columns\"\"\"\n",
    "    if df is None:\n",
    "        df = get_air_quality_data()\n",
    "    if df.empty:\n",
    "        raise ValueError('Data not loaded - the file is empty')\n",
    "    \n",
    "    # Check if target column exists\n",
    "    if target not in df.columns:\n",
    "        available_columns = list(df.columns)\n",
    "        raise ValueError(f'Target column \"{target}\" not found. Available columns: {available_columns}')\n",
    "\n",
    "    df.replace(-200, np.nan, inplace=True)\n",
    "    sensor_columns = df.columns.difference(['Date', 'Time'])\n",
    "    df.dropna(subset=sensor_columns, how='all', inplace=True)\n",
    "    df.dropna(subset=[target], inplace=True)\n",
    "    \n",
    "    if 'NMHC(GT)' in df.columns:\n",
    "        df.drop('NMHC(GT)', axis=1, inplace=True)\n",
    "        \n",
    "    # Check if we have enough data after cleaning\n",
    "    if len(df) == 0:\n",
    "        raise ValueError('No data remaining after cleaning')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d956b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_datetime_column(df, date_col='Date', time_col='Time'):\n",
    "    \"\"\"Combines separate 'Date' and 'Time' columns into a single datetime column\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    if date_col not in df.columns or time_col not in df.columns:\n",
    "        print(f\"Error: Missing columns. Please check if '{date_col}' and '{time_col}' exist.\")\n",
    "        return df\n",
    "    \n",
    "    df['DateTime'] = pd.to_datetime(\n",
    "        df[date_col] + ' ' + df[time_col], \n",
    "        format='%d/%m/%Y %H.%M.%S',\n",
    "        errors='coerce'\n",
    "    )\n",
    "    \n",
    "    df.drop(columns=[date_col, time_col], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76c4289",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_train_test_split(df, datetime_col='DateTime', test_months=3):\n",
    "    df = df.copy()\n",
    "    df['__year'] = df[datetime_col].dt.year\n",
    "    df['__month'] = df[datetime_col].dt.month\n",
    "    year_month_df = df[['__year', '__month']]\n",
    "    \n",
    "    unique_periods = year_month_df.drop_duplicates().sort_values(['__year', '__month'])\n",
    "    test_periods = unique_periods.tail(test_months)\n",
    "    \n",
    "    test_mask = year_month_df.apply(tuple, axis=1).isin(test_periods.apply(tuple, axis=1))\n",
    "    \n",
    "    train_idx=df.loc[~test_mask].index.tolist()\n",
    "    test_idx = df.loc[test_mask].index.tolist()\n",
    "    \n",
    "    return train_idx, test_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9c0bf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateTimeTransformer(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Custom transformer to extract and encode cyclical temporal features from 'Date' and 'Time' columns\"\"\"\n",
    "    def __init__(self, datetime_col='DateTime', use_day_of_year=True, use_year=True, use_day=True):\n",
    "        self.datetime_col = datetime_col\n",
    "        self.use_day_of_year = use_day_of_year\n",
    "        self.use_year = use_year\n",
    "        self.use_day = use_day\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        # Check if required column exist\n",
    "        if self.datetime_col not in X.columns:\n",
    "            raise ValueError(f\"Missing datetime column '{self.datetime_col}'\")\n",
    "        \n",
    "        dt_series = X[self.datetime_col]\n",
    "        \n",
    "        # Handle any parsing errors\n",
    "        if dt_series.isna().all():\n",
    "            raise ValueError(\"Could not parse datetime from Date and Time columns\")\n",
    "        \n",
    "        features = {\n",
    "            'Month_sin': np.sin(2 * np.pi * dt_series.dt.month / 12), \n",
    "            'Month_cos': np.cos(2 * np.pi * dt_series.dt.month / 12),\n",
    "            'DayOfWeek_sin': np.sin(2 * np.pi * dt_series.dt.dayofweek / 7),\n",
    "            'DayOfWeek_cos': np.cos(2 * np.pi * dt_series.dt.dayofweek / 7),\n",
    "            'Hour_sin': np.sin(2 * np.pi * dt_series.dt.hour / 24),\n",
    "            'Hour_cos': np.cos(2 * np.pi * dt_series.dt.hour / 24),\n",
    "        }\n",
    "\n",
    "        if self.use_day_of_year:\n",
    "            features['DayOfYear_sin'] = np.sin(2 * np.pi * dt_series.dt.dayofyear / 365)\n",
    "            features['DayOfYear_cos'] = np.cos(2 * np.pi * dt_series.dt.dayofyear / 365)      \n",
    "        if self.use_year:\n",
    "            features['Year'] = dt_series.dt.year\n",
    "        if self.use_day:\n",
    "            features['Day_sin'] = np.sin(2 * np.pi * dt_series.dt.day / 31)\n",
    "            features['Day_cos'] = np.cos(2 * np.pi * dt_series.dt.day / 31)\n",
    "\n",
    "        return pd.DataFrame(features, index=X.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3fdbf3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "dt_pipeline = Pipeline([\n",
    "    ('datetime', DateTimeTransformer()),\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25862c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'forest_reg': {\n",
    "        'estimator': RandomForestRegressor(random_state=42),\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'n_estimators': [200, 300, 400],\n",
    "                'max_features': ['log2', 'sqrt', 0.5, 0.6, 0.7, 1],\n",
    "                'max_depth': [None, 10, 20],\n",
    "                'min_samples_split': [2, 5, 10],\n",
    "                'min_samples_leaf': [1, 2, 4]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'estimator': xgb.XGBRegressor(random_state=42),\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'n_estimators': [200, 300, 400],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'learning_rate': [0.05, 0.1, 0.3],\n",
    "                'subsample': [0.7, 0.9, 1],\n",
    "                'colsample_bytree': [ 0.6, 0.8, 1],\n",
    "                'gamma': [0, 0.1, 0.3],\n",
    "                'reg_lambda': [1, 5, 10],\n",
    "                'reg_alpha': [0, 0.1, 0.5, 1],\n",
    "                'min_child_weight': [1, 5, 10, 20]\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    'hist_grad_boost': {\n",
    "        'estimator': HistGradientBoostingRegressor(random_state=42),\n",
    "        'param_grid': [\n",
    "            {\n",
    "                'max_iter': [200, 300, 400],\n",
    "                'max_depth': [4, 6, 8],\n",
    "                'max_features': [1, 'sqrt', 'log2', 0.8, 0.5],\n",
    "                'learning_rate': [0.05, 0.1, 0.3],\n",
    "                'l2_regularization': [1, 5, 10],\n",
    "                'min_samples_leaf': [1, 5, 10, 20]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c3438a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_models(models, X_train, y_train, full_pipeline, cv=TimeSeriesSplit(n_splits=10, gap=24), skipped_models=None):\n",
    "    \"\"\"Trains the specified models using RandomizedSearchCV and returns the trained models\"\"\"\n",
    "    trained_models = {}\n",
    "    skipped_models = skipped_models or []\n",
    "\n",
    "    for model_name, model_info in models.items():\n",
    "        if model_name in skipped_models:\n",
    "            continue\n",
    "        \n",
    "        print(f'Training model: {model_name.upper()}')\n",
    "\n",
    "        estimator_pipeline = Pipeline([\n",
    "            ('preprocessor', full_pipeline),\n",
    "            ('regressor', model_info['estimator'])\n",
    "        ])\n",
    "        \n",
    "        param_grid = {}\n",
    "        for param, values in model_info['param_grid'][0].items():\n",
    "            param_grid[f'regressor__{param}'] = values\n",
    "            \n",
    "        random_search = RandomizedSearchCV(\n",
    "            estimator_pipeline, param_distributions=param_grid, cv=cv,\n",
    "            scoring='neg_root_mean_squared_error', random_state=42,\n",
    "            return_train_score=True, n_jobs=1, n_iter=300, verbose=1\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            random_search.fit(X_train, y_train)\n",
    "            trained_models[model_name] = {\n",
    "                'search_cv': random_search,\n",
    "                'best_estimator': random_search.best_estimator_\n",
    "            }\n",
    "            print(f'{model_name.upper()}: training completed')\n",
    "        except Exception as e:\n",
    "            print(f'Error during training {model_name}: {e}')\n",
    "            continue\n",
    "            \n",
    "    return trained_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4428c044",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(trained_models, save_dir='models', leaderboard_flag=True):\n",
    "    \"\"\"Saves the trained models and optionally generates a leaderboard\"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    leaderboard = []\n",
    "\n",
    "    for model_name, model_info in trained_models.items():\n",
    "        model_path = os.path.join(save_dir, model_name)\n",
    "        os.makedirs(model_path, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            joblib.dump(model_info['search_cv'], os.path.join(model_path, 'search_cv.joblib'))\n",
    "            joblib.dump(model_info['best_estimator'], os.path.join(model_path, 'best_model.joblib'))\n",
    "            print(f'{model_name}: saved')\n",
    "\n",
    "            if leaderboard_flag:\n",
    "                leaderboard.append({\n",
    "                    'Model': model_name,\n",
    "                    'CV_RMSE': round(-model_info['search_cv'].best_score_, 4),\n",
    "                    'Best_Params': str(model_info['search_cv'].best_params_)\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f'Error during saving {model_name}: {e}')\n",
    "\n",
    "    if leaderboard_flag and leaderboard:\n",
    "        leaderboard_df = pd.DataFrame(leaderboard).sort_values('CV_RMSE').reset_index(drop=True)\n",
    "        leaderboard_df.to_csv(os.path.join(save_dir, 'leaderboard.csv'), index=False)\n",
    "        print(f'\\nLeaderboard saved to {os.path.join(save_dir, \"leaderboard.csv\")}')\n",
    "    elif not leaderboard_flag:\n",
    "        print('\\nLeaderboard generation skipped')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8713834",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(models, save_dir):\n",
    "    \"\"\"Loads trained models from the specified directory\"\"\"\n",
    "    loaded_models = {}\n",
    "    for model_name in models.keys():\n",
    "        search_cv_path = os.path.join(save_dir, model_name, 'search_cv.joblib')\n",
    "        best_model_path = os.path.join(save_dir, model_name, 'best_model.joblib')\n",
    "\n",
    "        if os.path.exists(search_cv_path) and os.path.exists(best_model_path):\n",
    "            try:\n",
    "                loaded_models[model_name] = {\n",
    "                    'search_cv': joblib.load(search_cv_path),\n",
    "                    'best_estimator': joblib.load(best_model_path)\n",
    "                }\n",
    "                print(f'{model_name}: loaded from disk')\n",
    "            except Exception as e:\n",
    "                print(f'Error loading model {model_name}: {e}')\n",
    "    return loaded_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8827a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_save_models(models, X_train, y_train, full_pipeline, save_dir='models', leaderboard_flag=True, cv=None, force_retrain=False):\n",
    "    \"\"\"Trains and saves the specified models, optionally generating a leaderboard\"\"\"\n",
    "    if cv is None:\n",
    "        cv = TimeSeriesSplit(n_splits=10, gap=24)\n",
    "\n",
    "    loaded_models = load_models(models, save_dir)\n",
    "    skipped_model_names = list(loaded_models.keys()) if not force_retrain else []\n",
    "\n",
    "    if force_retrain:\n",
    "        print('\\nForce retrain is ON â€” all models will be retrained.\\n')\n",
    "        loaded_models = {}\n",
    "        skipped_model_names = []\n",
    "    elif skipped_model_names:\n",
    "        print('\\nAlready trained models found and will be skipped:')\n",
    "        for model in skipped_model_names:\n",
    "            print(f'  - {model}')\n",
    "\n",
    "    trained_models = train_models(\n",
    "        models, X_train, y_train, full_pipeline,\n",
    "        cv=cv, skipped_models=skipped_model_names\n",
    "    )\n",
    "            \n",
    "    all_models = {**loaded_models, **trained_models}\n",
    "    save_models(all_models, save_dir=save_dir, leaderboard_flag=leaderboard_flag)\n",
    "    return all_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad61dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data with shape (9471, 15) and columns: ['Date', 'Time', 'CO(GT)', 'PT08.S1(CO)', 'NMHC(GT)', 'C6H6(GT)', 'PT08.S2(NMHC)', 'NOx(GT)', 'PT08.S3(NOx)', 'NO2(GT)', 'PT08.S4(NO2)', 'PT08.S5(O3)', 'T', 'RH', 'AH']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator DecisionTreeRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomForestRegressor from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\base.py:442: InconsistentVersionWarning: Trying to unpickle estimator RandomizedSearchCV from version 1.5.1 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[47]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Create preprocessing pipeline\u001b[39;00m\n\u001b[32m     31\u001b[39m full_pipeline = ColumnTransformer([\n\u001b[32m     32\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mnum\u001b[39m\u001b[33m'\u001b[39m, num_pipeline, air_quality_num_columns),\n\u001b[32m     33\u001b[39m     (\u001b[33m'\u001b[39m\u001b[33mdt\u001b[39m\u001b[33m'\u001b[39m, dt_pipeline, air_quality_dt_columns) \n\u001b[32m     34\u001b[39m ])\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m trained_models = \u001b[43mtrain_save_models\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfull_pipeline\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmodels\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m    \u001b[49m\u001b[43mleaderboard_flag\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m=\u001b[49m\u001b[43mTimeSeriesSplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgap\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m24\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_retrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m     45\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     47\u001b[39m test_results = []\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model_info \u001b[38;5;129;01min\u001b[39;00m trained_models.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[46]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mtrain_save_models\u001b[39m\u001b[34m(models, X_train, y_train, full_pipeline, save_dir, leaderboard_flag, cv, force_retrain)\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m      4\u001b[39m     cv = TimeSeriesSplit(n_splits=\u001b[32m10\u001b[39m, gap=\u001b[32m24\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m loaded_models = \u001b[43mload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m skipped_model_names = \u001b[38;5;28mlist\u001b[39m(loaded_models.keys()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m force_retrain \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m force_retrain:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[45]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mload_models\u001b[39m\u001b[34m(models, save_dir)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(search_cv_path) \u001b[38;5;129;01mand\u001b[39;00m os.path.exists(best_model_path):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     10\u001b[39m         loaded_models[model_name] = {\n\u001b[32m     11\u001b[39m             \u001b[33m'\u001b[39m\u001b[33msearch_cv\u001b[39m\u001b[33m'\u001b[39m: joblib.load(search_cv_path),\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mbest_estimator\u001b[39m\u001b[33m'\u001b[39m: \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m         }\n\u001b[32m     14\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: loaded from disk\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:749\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(filename, mmap_mode, ensure_native_byte_order)\u001b[39m\n\u001b[32m    744\u001b[39m                 \u001b[38;5;28;01mreturn\u001b[39;00m load_compatibility(fobj)\n\u001b[32m    746\u001b[39m             \u001b[38;5;66;03m# A memory-mapped array has to be mapped with the endianness\u001b[39;00m\n\u001b[32m    747\u001b[39m             \u001b[38;5;66;03m# it has been written with. Other arrays are coerced to the\u001b[39;00m\n\u001b[32m    748\u001b[39m             \u001b[38;5;66;03m# native endianness of the host system.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m             obj = \u001b[43m_unpickle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    750\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[43m                \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    752\u001b[39m \u001b[43m                \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    753\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmmap_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvalidated_mmap_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    754\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    756\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:626\u001b[39m, in \u001b[36m_unpickle\u001b[39m\u001b[34m(fobj, ensure_native_byte_order, filename, mmap_mode)\u001b[39m\n\u001b[32m    624\u001b[39m obj = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    625\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m     obj = \u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    627\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m unpickler.compat_mode:\n\u001b[32m    628\u001b[39m         warnings.warn(\n\u001b[32m    629\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe file \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m has been generated with a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    630\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mjoblib version less than 0.10. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    633\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    634\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\anaconda3\\Lib\\pickle.py:1255\u001b[39m, in \u001b[36m_Unpickler.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1253\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEOFError\u001b[39;00m\n\u001b[32m   1254\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, bytes_types)\n\u001b[32m-> \u001b[39m\u001b[32m1255\u001b[39m         \u001b[43mdispatch\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1256\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _Stop \u001b[38;5;28;01mas\u001b[39;00m stopinst:\n\u001b[32m   1257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m stopinst.value\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:462\u001b[39m, in \u001b[36mNumpyUnpickler.load_build\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    460\u001b[39m     _array_payload = array_wrapper.read(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     _array_payload = \u001b[43marray_wrapper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    464\u001b[39m \u001b[38;5;28mself\u001b[39m.stack.append(_array_payload)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:284\u001b[39m, in \u001b[36mNumpyArrayWrapper.read\u001b[39m\u001b[34m(self, unpickler, ensure_native_byte_order)\u001b[39m\n\u001b[32m    282\u001b[39m     array = \u001b[38;5;28mself\u001b[39m.read_mmap(unpickler)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     array = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpickler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_native_byte_order\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# Manage array subclass case\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(array, \u001b[33m\"\u001b[39m\u001b[33m__array_prepare__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.subclass \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[32m    288\u001b[39m     unpickler.np.ndarray,\n\u001b[32m    289\u001b[39m     unpickler.np.memmap,\n\u001b[32m    290\u001b[39m ):\n\u001b[32m    291\u001b[39m     \u001b[38;5;66;03m# We need to reconstruct another subclass\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle.py:197\u001b[39m, in \u001b[36mNumpyArrayWrapper.read_array\u001b[39m\u001b[34m(self, unpickler, ensure_native_byte_order)\u001b[39m\n\u001b[32m    195\u001b[39m read_count = \u001b[38;5;28mmin\u001b[39m(max_read_count, count - i)\n\u001b[32m    196\u001b[39m read_size = \u001b[38;5;28mint\u001b[39m(read_count * \u001b[38;5;28mself\u001b[39m.dtype.itemsize)\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m data = \u001b[43m_read_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43munpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfile_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mread_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43marray data\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    198\u001b[39m array[i : i + read_count] = unpickler.np.frombuffer(\n\u001b[32m    199\u001b[39m     data, dtype=\u001b[38;5;28mself\u001b[39m.dtype, count=read_count\n\u001b[32m    200\u001b[39m )\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\joblib\\numpy_pickle_utils.py:266\u001b[39m, in \u001b[36m_read_bytes\u001b[39m\u001b[34m(fp, size, error_template)\u001b[39m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;66;03m# io files (default in python3) return None or raise on\u001b[39;00m\n\u001b[32m    263\u001b[39m     \u001b[38;5;66;03m# would-block, python2 file will truncate, probably nothing can be\u001b[39;00m\n\u001b[32m    264\u001b[39m     \u001b[38;5;66;03m# done about that.  note that regular files can't be non-blocking\u001b[39;00m\n\u001b[32m    265\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m         r = \u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    267\u001b[39m         data += r\n\u001b[32m    268\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(r) == \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) == size:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    try:\n",
    "        # Load and clean data\n",
    "        air_quality = get_air_quality_clean_data()\n",
    "        air_quality = add_datetime_column(air_quality)\n",
    "        target = 'C6H6(GT)'\n",
    "\n",
    "        X = air_quality.drop(target, axis=1)\n",
    "        y = air_quality[target]\n",
    "        \n",
    "        # Split indices\n",
    "        train_idx, test_idx = seasonal_train_test_split(\n",
    "            air_quality, datetime_col='DateTime',  test_months=3\n",
    "        )\n",
    "        \n",
    "        # Prepare train and test sets: combine X and y, sort by DateTime, and reset indices \n",
    "        train = pd.concat([X.loc[train_idx].copy(), y.loc[train_idx].copy()], axis=1)\n",
    "        train = train.sort_values('DateTime').reset_index(drop=True)\n",
    "        X_train = train.drop(target, axis=1)\n",
    "        y_train = train[target]\n",
    "        \n",
    "        test = pd.concat([X.loc[test_idx].copy(), y.loc[test_idx].copy()], axis=1)\n",
    "        test = test.sort_values('DateTime').reset_index(drop=True)\n",
    "        X_test = test.drop(target, axis=1)\n",
    "        y_test = test[target]\n",
    "        \n",
    "        air_quality_num_columns = X_train.select_dtypes(include=[np.number]).columns.tolist()\n",
    "        air_quality_dt_columns = ['DateTime']\n",
    "        \n",
    "        # Create preprocessing pipeline\n",
    "        full_pipeline = ColumnTransformer([\n",
    "            ('num', num_pipeline, air_quality_num_columns),\n",
    "            ('dt', dt_pipeline, air_quality_dt_columns) \n",
    "        ])\n",
    "\n",
    "        trained_models = train_save_models(\n",
    "            models,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            full_pipeline,\n",
    "            save_dir='models',\n",
    "            leaderboard_flag=True,\n",
    "            cv=TimeSeriesSplit(n_splits=10, gap=24),\n",
    "            force_retrain=True\n",
    "        )\n",
    "        \n",
    "        test_results = []\n",
    "        for model_name, model_info in trained_models.items():\n",
    "            try:\n",
    "                best_model = model_info['best_estimator']\n",
    "                y_pred = best_model.predict(X_test)\n",
    "                rmse = root_mean_squared_error(y_test, y_pred)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                test_results.append({\n",
    "                    'Model': model_name,\n",
    "                    'RMSE': round(rmse, 4), \n",
    "                    'R2Score': round(r2, 4)\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "\n",
    "        test_results_df = pd.DataFrame(test_results).sort_values('RMSE').reset_index(drop=True)\n",
    "        test_results_df.to_csv('models/test_results.csv', index=False) # Save test results\n",
    "        test_results_df\n",
    "    except Exception as e:\n",
    "        print(f'An error occurred: {e}')\n",
    "        traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
